{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0dd70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mic opened for 2sâ€¦\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyaudio\n",
    "p = pyaudio.PyAudio()\n",
    "s = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)\n",
    "print(\"Mic opened for 2sâ€¦\")\n",
    "time.sleep(2)\n",
    "s.stop_stream(); s.close(); p.terminate()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58570580-a329-441b-bc02-f74bd7cafbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v2 as speech\n",
    "client = speech.SpeechClient(\n",
    "    client_options={\"api_endpoint\": \"us-central1-speech.googleapis.com\"}\n",
    ")\n",
    "\n",
    "# stt_stream_v2.py\n",
    "import queue, sys, pyaudio\n",
    "from google.cloud import speech_v2 as speech\n",
    "\n",
    "PROJECT  = \"map-mcp-471616\"\n",
    "LOCATION = \"us-central1\"\n",
    "LANG     = \"en-US\"\n",
    "RATE     = 16000\n",
    "CHUNK    = int(RATE * 0.05)  # 50 ms\n",
    "\n",
    "client = speech.SpeechClient(\n",
    "    client_options={\"api_endpoint\": f\"{LOCATION}-speech.googleapis.com\"}\n",
    ")\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    auto_decoding_config=speech.AutoDetectDecodingConfig(),  # raw PCM from mic is fine\n",
    "    features=speech.RecognitionFeatures(enable_automatic_punctuation=True),\n",
    "    language_codes=[LANG],\n",
    "    model=\"latest_short\",  # good for live utterances\n",
    ")\n",
    "\n",
    "streaming_cfg = speech.StreamingRecognitionConfig(\n",
    "    config=config,\n",
    "    streaming_features=speech.StreamingRecognitionFeatures(\n",
    "        interim_results=True, enable_voice_activity_events=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "recognizer = f\"projects/{PROJECT}/locations/{LOCATION}/recognizers/_\"\n",
    "\n",
    "\n",
    "\n",
    "# --- mic helper ---\n",
    "q = queue.Queue()\n",
    "pa = pyaudio.PyAudio()\n",
    "\n",
    "stream = pa.open(format=pyaudio.paInt16, channels=1, rate=RATE, input=True,\n",
    "                 frames_per_buffer=CHUNK,\n",
    "                 stream_callback=lambda in_data, *_: (q.put(in_data), pyaudio.paContinue)[1])\n",
    "\n",
    "def request_gen():\n",
    "    # first message: recognizer + config (no audio)\n",
    "    yield speech.StreamingRecognizeRequest(\n",
    "        recognizer=recognizer, streaming_config=streaming_cfg\n",
    "    )\n",
    "    # subsequent messages: audio only\n",
    "    while True:\n",
    "        yield speech.StreamingRecognizeRequest(audio=q.get())\n",
    "\n",
    "print(\"ðŸŽ¤ Speak (Ctrl+C to stop)â€¦\")\n",
    "try:\n",
    "    stream.start_stream()\n",
    "    responses = client.streaming_recognize(requests=request_gen())\n",
    "    for resp in responses:\n",
    "        for res in resp.results:\n",
    "            if not res.alternatives: continue\n",
    "            text = res.alternatives[0].transcript.strip()\n",
    "            if res.is_final:\n",
    "                print(f\"\\nâœ… {text}\")\n",
    "            else:\n",
    "                sys.stdout.write(f\"\\râ€¦ {text} \"); sys.stdout.flush()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    stream.stop_stream(); stream.close(); pa.terminate()\n",
    "    print(\"\\nStopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116eec1-f420-422d-a93e-c8f5461e2531",
   "metadata": {},
   "source": [
    "# Streaming method starts from here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6514ad-aea5-4495-930c-a0ac81c91a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759710298.902614 43054839 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Speak (Ctrl+C to stop)â€¦\n",
      "â€¦ Hello. \n",
      "âœ… Hello.\n",
      "â€¦ I hope you don't do this twice. \n",
      "âœ… I hope you don't do this twice.\n",
      "â€¦ done. e. e. \n",
      "âœ… It is done.\n",
      "\n",
      "Stopped.\n"
     ]
    }
   ],
   "source": [
    "# stt_stream_v2.py â€” revised, callback-based and Jupyter-friendly\n",
    "import sys\n",
    "import queue\n",
    "import pyaudio\n",
    "from google.cloud import speech_v2 as speech\n",
    "\n",
    "# --- config you should set ---\n",
    "PROJECT  = \"map-mcp-471616\"\n",
    "LOCATION = \"us-central1\"     # choose your region, e.g., us-central1 / europe-west1 / asia-southeast1\n",
    "LANG     = \"en-US\"           # BCP-47 code\n",
    "RATE     = 16000             # mic sample rate (Hz)\n",
    "CHUNK    = int(RATE * 0.05)  # 50 ms of audio per callback\n",
    "# -----------------------------\n",
    "\n",
    "# v2 requires a regional endpoint\n",
    "client = speech.SpeechClient(\n",
    "    client_options={\"api_endpoint\": f\"{LOCATION}-speech.googleapis.com\"}\n",
    ")\n",
    "\n",
    "# Weâ€™re streaming raw PCM (LINEAR16) from the mic, so use explicit decoding.\n",
    "config = speech.RecognitionConfig(\n",
    "    explicit_decoding_config=speech.ExplicitDecodingConfig(\n",
    "        encoding=\"LINEAR16\",\n",
    "        sample_rate_hertz=RATE,\n",
    "        audio_channel_count=1,\n",
    "    ),\n",
    "    features=speech.RecognitionFeatures(enable_automatic_punctuation=True),\n",
    "    language_codes=[LANG],\n",
    "    model=\"latest_long\",\n",
    ")\n",
    "\n",
    "streaming_cfg = speech.StreamingRecognitionConfig(\n",
    "    config=config,\n",
    "    streaming_features=speech.StreamingRecognitionFeatures(\n",
    "        interim_results=True,\n",
    "        enable_voice_activity_events=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "recognizer = f\"projects/{PROJECT}/locations/{LOCATION}/recognizers/_\"\n",
    "\n",
    "# --- microphone stream & callback ---\n",
    "q: \"queue.Queue[bytes]\" = queue.Queue()\n",
    "pa = pyaudio.PyAudio()\n",
    "\n",
    "def _callback(in_data, frame_count, time_info, status_flags):\n",
    "    \"\"\"\n",
    "    PyAudio calls this whenever a CHUNK of mic audio is available.\n",
    "    - in_data: raw bytes (LINEAR16, little-endian), len = frame_count * 2 bytes (mono, 16-bit)\n",
    "    - Return (out_data, flag). For input-only, out_data is None.\n",
    "    \"\"\"\n",
    "    q.put(in_data)                      # hand off audio to the gRPC request generator\n",
    "    return (None, pyaudio.paContinue)   # keep the stream running\n",
    "\n",
    "stream = pa.open(\n",
    "    format=pyaudio.paInt16,\n",
    "    channels=1,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=CHUNK,\n",
    "    stream_callback=_callback,\n",
    ")\n",
    "\n",
    "def cleanup_the_stream():\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "    print(\"\\nStopped.\")\n",
    "    \n",
    "def request_generator():\n",
    "    \"\"\"Yield the initial config request, then a stream of audio-chunk requests.\"\"\"\n",
    "    # 1) first message: recognizer + streaming config, no audio\n",
    "    yield speech.StreamingRecognizeRequest(\n",
    "        recognizer=recognizer,\n",
    "        streaming_config=streaming_cfg,\n",
    "    )\n",
    "    # 2) subsequent messages: raw audio bytes\n",
    "    while True:\n",
    "        chunk = q.get()\n",
    "        if chunk is None:\n",
    "            break\n",
    "        yield speech.StreamingRecognizeRequest(audio=chunk)\n",
    "\n",
    "class ExitLoop(Exception):\n",
    "    pass\n",
    "\n",
    "print(\"ðŸŽ¤ Speak (Ctrl+C to stop)â€¦\")\n",
    "try:\n",
    "    stream.start_stream()\n",
    "    responses = client.streaming_recognize(requests=request_generator())\n",
    "\n",
    "    for resp in responses:\n",
    "        # (optional) voice activity events are available in resp.speech_event_type\n",
    "        for result in resp.results:\n",
    "            if not result.alternatives:\n",
    "                continue\n",
    "            text = result.alternatives[0].transcript.strip()\n",
    "            if result.is_final:\n",
    "                print(f\"\\nâœ… {text}\")\n",
    "                if text == \"It is done.\":\n",
    "                    raise ExitLoop()\n",
    "            else:\n",
    "                # overwrite the same line for interim hypotheses\n",
    "                sys.stdout.write(f\"\\râ€¦ {text} \")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "except ExitLoop:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "    print(\"\\nStopped.\")    \n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a18b96b-f072-4307-9dc0-d4651a62afdf",
   "metadata": {},
   "source": [
    "stream.stop_stream()\n",
    "stream.close()\n",
    "pa.terminate()\n",
    "print(\"\\nStopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7c5d2-e03e-4f37-896d-618315a3883f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vca_venv)",
   "language": "python",
   "name": "vca_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
